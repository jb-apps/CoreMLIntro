{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f49c3ad-8cb3-4317-832c-1224477e03c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch version 2.3.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.2.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a533a26-db8e-4718-9d70-a8fbd7af7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from basicsr.archs.srvgg_arch import SRVGGNetCompact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19472c58-f975-4835-ad8e-0428e9b5648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Pytorch model we will convert to CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ccf3c4-b35b-48f2-b43e-5deb63dd849c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SRVGGNetCompact(\n",
       "  (body): ModuleList(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): PReLU(num_parameters=64)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): PReLU(num_parameters=64)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): PReLU(num_parameters=64)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): PReLU(num_parameters=64)\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): PReLU(num_parameters=64)\n",
       "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): PReLU(num_parameters=64)\n",
       "    (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): PReLU(num_parameters=64)\n",
       "    (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): PReLU(num_parameters=64)\n",
       "    (16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): PReLU(num_parameters=64)\n",
       "    (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): PReLU(num_parameters=64)\n",
       "    (20): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): PReLU(num_parameters=64)\n",
       "    (22): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): PReLU(num_parameters=64)\n",
       "    (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): PReLU(num_parameters=64)\n",
       "    (26): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): PReLU(num_parameters=64)\n",
       "    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): PReLU(num_parameters=64)\n",
       "    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): PReLU(num_parameters=64)\n",
       "    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): PReLU(num_parameters=64)\n",
       "    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): PReLU(num_parameters=64)\n",
       "    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): PReLU(num_parameters=64)\n",
       "    (38): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (39): PReLU(num_parameters=64)\n",
       "    (40): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): PReLU(num_parameters=64)\n",
       "    (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (43): PReLU(num_parameters=64)\n",
       "    (44): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (45): PReLU(num_parameters=64)\n",
       "    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): PReLU(num_parameters=64)\n",
       "    (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (49): PReLU(num_parameters=64)\n",
       "    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (51): PReLU(num_parameters=64)\n",
       "    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (53): PReLU(num_parameters=64)\n",
       "    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (55): PReLU(num_parameters=64)\n",
       "    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (57): PReLU(num_parameters=64)\n",
       "    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (59): PReLU(num_parameters=64)\n",
       "    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (61): PReLU(num_parameters=64)\n",
       "    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (63): PReLU(num_parameters=64)\n",
       "    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (65): PReLU(num_parameters=64)\n",
       "    (66): Conv2d(64, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (upsampler): PixelShuffle(upscale_factor=4)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_path = 'realesr-general-x4v3.pth'\n",
    "\n",
    "model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n",
    "\n",
    "loadnet = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(loadnet['params'], strict=True)\n",
    "model.train(False)\n",
    "model.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e647cb36-e6e2-4234-adaf-0d739a125f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63aff8f2-3d64-470c-82c1-5683e5670214",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_trace = (1, 3, 512, 512)\n",
    "input_shape = (1, 3, 512, 512)\n",
    "\n",
    "example_input = torch.rand(input_trace)\n",
    "\n",
    "# Trace the model\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "input_image = ct.ImageType(shape=input_shape, color_layout=ct.colorlayout.RGB, scale=1/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93e9d0c-f4f0-4e6b-9770-92d7e90b3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the traced model to Core ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3ef4543-d1f6-4174-8aed-549be55127e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 347/348 [00:00<00:00, 10788.75 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 363.64 passes/s]\n",
      "Running MIL default pipeline: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76/76 [00:00<00:00, 342.35 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 437.00 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344/344 [00:00<00:00, 580.13 ops/s]\n"
     ]
    }
   ],
   "source": [
    "# Define input / output types\n",
    "mlmodel = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=[input_image], \n",
    "    outputs=[ct.ImageType(color_layout=ct.colorlayout.RGB)],\n",
    "    convert_to=\"neuralnetwork\"\n",
    ")\n",
    "\n",
    "mlmodel.save(\"Anime.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e51ec21b-214b-425e-af72-54779979ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding output as image\n",
    "from coremltools.models.neural_network import NeuralNetworkBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3f1fff7-c6e1-4335-b4cf-ed958a312e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel.save(\"Anime.mlmodel\")\n",
    "\n",
    "############## FIX OUTPUT ######################\n",
    "\n",
    "# Load the Core ML model\n",
    "spec = ct.utils.load_spec(\"Anime.mlmodel\")\n",
    "\n",
    "# Create a builder from the existing spec\n",
    "builder = NeuralNetworkBuilder(spec=spec)\n",
    "\n",
    "# Get the name of the last layer in the model\n",
    "last_layer = builder.spec.neuralNetwork.layers[-1].output[0]\n",
    "\n",
    "# Add an ActivationLinear layer to scale the output\n",
    "builder.add_activation(name=\"scaled\",\n",
    "                       non_linearity=\"LINEAR\",\n",
    "                       input_name=last_layer,\n",
    "                       output_name=\"scaled\",\n",
    "                       params=[255.0, 0.0])  # Params for the LINEAR activation function (alpha, beta)\n",
    "\n",
    "# Add a Squeeze layer after the scaling layer\n",
    "\n",
    "builder.add_squeeze(name=\"enhanced\",\n",
    "                    input_name=\"scaled\",\n",
    "                    output_name=\"enhanced\",\n",
    "                    axes=[0])\n",
    "\n",
    "# Update the output of the model to be the output of the squeeze layer\n",
    "builder.spec.description.output[0].name = 'enhanced'\n",
    "\n",
    "# Save the modified model\n",
    "ct.utils.save_spec(builder.spec, \"Anime.mlmodel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
